---
layout: post
title:  "ZooKeeper源码阅读(7)-服务端处理请求（内存数据库）"
date:   2019-01-01 03:49:45 +0200
categories: ZooKeeper 源码
---
上篇博客介绍了ZooKeeperServer的责任链模式，本文将探索ZK在内存中是如何维护那些数据的(即目录结构)。

对应的类为ZKDatabase, 首先来看类的注释：
> This class maintains the in memory database of zookeeper
  server states that includes the sessions, datatree and the
  committed logs. It is booted up  after reading the logs
  and snapshots from the disk.

上述注释告诉我们这个类维护了内存数据库状态，以及会话，日志等相关信息。
首先来看这个类是如何初始化的，在ZooKeeperServer的loadData()函数中，该类被初始化：

{% highlight java %}
public void loadData() throws IOException, InterruptedException {
    if(zkDb.isInitialized()){
        setZxid(zkDb.getDataTreeLastProcessedZxid());
    }
    else {
        setZxid(zkDb.loadDataBase());
    }
}
{% endhighlight %}
可以看到，如果zkDb还没有初始化过，则调用loadDataBase()函数进行初始化：
{% highlight java %}
public long loadDataBase() throws IOException {
        long startTime = Time.currentElapsedTime();
        long zxid = snapLog.restore(dataTree, sessionsWithTimeouts, commitProposalPlaybackListener);
        initialized = true;
        long loadTime = Time.currentElapsedTime() - startTime;
        ServerMetrics.DB_INIT_TIME.add(loadTime);
        LOG.info("Snapshot loaded in " + loadTime + " ms");
        return zxid;
    }
{% endhighlight %}
果不其然，初始化肯定是利用日志文件进行恢复，即snapLog.restore()函数：
{% highlight java %}
public long restore(DataTree dt, Map<Long, Integer> sessions,
                    PlayBackListener listener) throws IOException {
    //从snapshot中恢复（snapshot是定期做的，不一定含有最新的数据）
    long deserializeResult = snapLog.deserialize(dt, sessions);
    //other logic
    //再从日志文件中恢复最新的数据
    return fastForwardFromEdits(dt, sessions, listener);
}
{% endhighlight %}
主要是分成两步，首先是从snapshot中恢复，主要就是snapshot文件的反序列化,其次因为snapshot文件中不一定含有最新数据，所有还有必要从日志文件中进行恢复，主要逻辑就是根据日志进行replay,重新把那些事务执行一遍，具体不再进一步分析。

来看一下该类重要的一些成员变量：
{% highlight java %}
protected DataTree dataTree; //树型结构，用来维护目录结构（目录就是一棵树）
protected ConcurrentHashMap<Long, Integer> sessionsWithTimeouts;//用来维护会话的超时逻辑
protected FileTxnSnapLog snapLog; //snap日志相关
protected long minCommittedLog, maxCommittedLog;//最小提交日志编号，最大提交日志编号，follower同步相关
{% endhighlight %}

上篇博客介绍到FinalRequestProcessor中最后是调用ZooKeeperServer的processTxn()函数来进行处理，跟踪其代码，最后发现是调用ZKDatabase的processTxn()函数：
{% highlight java %}
public ProcessTxnResult processTxn(TxnHeader hdr, Record txn) {
    return dataTree.processTxn(hdr, txn);
}
{% endhighlight %}
发现最后绕来绕去回到dataTree的processTxn函数上，那么有必要来看一下dataTree的实现。
{% highlight java %}
public class DataTree {
    /**
     * This hashtable provides a fast lookup to the datanodes. The tree is the
     * source of truth and is where all the locking occurs
     */
    private final ConcurrentHashMap<String, DataNode> nodes =
            new ConcurrentHashMap<String, DataNode>();
}
{% endhighlight %}
上述抽取了该类中最重要的成员变量，可以看到该类维护了一个ConcurrentHashMap来作为内存数据库来记录数据，其中key为Path, value为对应的数据节点，需要注意的是，这里的key都是完整的路径，例如我们在ZooKeeper中看到目录 /zk 和 /zk/child1, 则 该map的两个key分别为"/zk"和"/zk/child1", 我想key为完整路径的主要原因还是出于性能考虑，可以直接根据路径得到其对应的dataNode,这样就不用遍历那棵树了。

还是上篇博客的例子，假设ZooKeeper中已有/zk路径，我们的请求是创建路径 "/zk/child1"
{% highlight java %}
public ProcessTxnResult processTxn(TxnHeader header, Record txn, boolean isSubTxn)
    {
        ProcessTxnResult rc = new ProcessTxnResult();

        try {
            rc.clientId = header.getClientId();
            rc.cxid = header.getCxid();
            rc.zxid = header.getZxid();
            rc.type = header.getType();
            rc.err = 0;
            rc.multiResult = null;
            switch (header.getType()) {
                case OpCode.create:
                    CreateTxn createTxn = (CreateTxn) txn;
                    rc.path = createTxn.getPath();
                    createNode(
                            createTxn.getPath(),
                            createTxn.getData(),
                            createTxn.getAcl(),
                            createTxn.getEphemeral() ? header.getClientId() : 0,
                            createTxn.getParentCVersion(),
                            header.getZxid(), header.getTime(), null);
                    break;
                //other cases
            }
        } catch (Exception e) {
            // handle exception
        }
        return rc;
    }
{% endhighlight %}
上面抽取了和create请求有关的主要代码，可以看到，主要是从txn中获取参数，然后调用createNode()函数，而该函数则是真正的修改内存数据库的函数了。
{% highlight java %}
public void createNode(final String path, byte data[], List<ACL> acl,
        long ephemeralOwner, int parentCVersion, long zxid, long time, Stat outputStat)
        throws KeeperException.NoNodeException,
        KeeperException.NodeExistsException {
    int lastSlash = path.lastIndexOf('/');
    String parentName = path.substring(0, lastSlash);
    String childName = path.substring(lastSlash + 1);
    StatPersisted stat = new StatPersisted();
    stat.setCtime(time);
    stat.setMtime(time);
    stat.setCzxid(zxid);
    stat.setMzxid(zxid);
    stat.setPzxid(zxid);
    stat.setVersion(0);
    stat.setAversion(0);
    stat.setEphemeralOwner(ephemeralOwner);
    DataNode parent = nodes.get(parentName);
    if (parent == null) {
        throw new KeeperException.NoNodeException();
    }
    synchronized (parent) {
        Set<String> children = parent.getChildren();
        if (children.contains(childName)) {
            throw new KeeperException.NodeExistsException();
        }

        if (parentCVersion == -1) {
            parentCVersion = parent.stat.getCversion();
            parentCVersion++;
        }
        parent.stat.setCversion(parentCVersion);
        parent.stat.setPzxid(zxid);
        Long longval = aclCache.convertAcls(acl);
        DataNode child = new DataNode(data, longval, stat);
        parent.addChild(childName);
        nodeDataSize.addAndGet(getNodeSize(path, child.data));
        nodes.put(path, child);
        //other logic
        if (outputStat != null) {
          child.copyStat(outputStat);
        }
    }

    dataWatches.triggerWatch(path, Event.EventType.NodeCreated);
    childWatches.triggerWatch(parentName.equals("") ? "/" : parentName,
            Event.EventType.NodeChildrenChanged);
}
{% endhighlight %}
上述代码也是比较好理解的，首先根据参数新节点的status,然后根据路径得到其父节点，然后锁住其父节点（这样就保证了不会有并发的子节点插入）,再插入子节点，同时调整父节点的数据。 还是刚才的例子，假设ZooKeeper中已有/zk路径，我们的请求是创建路径 "/zk/child1"，首先会得到父节点"/zk",然后父节点中会记录有一个子节点叫做child1,注意子节点是保存的相对路径，这样就很容易得到子节点的完整路径。具体参考DataNode的定义：
{% highlight java %}
public class DataNode implements Record {
    /**
     * the data for this datanode
     */
    byte data[];

    /**
     * the acl map long for this datanode. the datatree has the map
     */
    Long acl;

    /**
     * the stat for this node that is persisted to disk.
     */
    public StatPersisted stat;

    /**
     * the list of children for this node. note that the list of children string
     * does not contain the parent path -- just the last part of the path. This
     * should be synchronized on except deserializing (for speed up issues).
     */
    private Set<String> children = null;
}
{% endhighlight %}
其中的注释写的非常清楚。

这样我们的请求就已经反应在ConcurrentHashMap里面了，也就是所谓的内存数据库，到这我们也完成了单机版请求的主要处理过程。
接下来我们就要分析在集群状态下的情况了，很多情况和单机版类似，故我们只分析集群特殊的部分，如leader选举啊，leader和follower的数据同步啊以及集群状态的责任链模式等等。


------------------------------------
ZooKeeper 源码地址：[https://github.com/apache/zookeeper](https://github.com/apache/zookeeper)

参考书籍：《从Paxos到Zookeeper 分布式一致性原理与实践》
